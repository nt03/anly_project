{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in dcra_cama merged data csv\n",
    "df = pd.read_csv(\"cleaned_merged_data5.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max number of review cycles for a ssl\n",
    "#maxRC = df.groupby([\"ssl\"],  sort= False)['ReviewCycle'].max()\n",
    "#maxRC = maxRC.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean number of review cycles for a ssl rounded to nearest integer\n",
    "maxRC = df.groupby([\"ssl\"],  sort= False)['ReviewCycle'].aggregate(np.mean).round(0)\n",
    "maxRC = maxRC.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxRC.shape\n",
    "maxRC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean time taken per ssl\n",
    "#time = df.groupby([\"ssl\"],  sort= False)['elapsed_workdays'].aggregate(np.mean).round(3)\n",
    "#time = time.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(time.shape)\n",
    "#time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new df combining same depts with varying reviewCycle\n",
    "dfnew = df.groupby(['ssl', 'GroupName', 'Ward','use_type', 'PRICE', 'QUALIFIED', 'EYB', 'SALE_NUM', 'LIVING_GBA', 'USECODE',\n",
    "       'LANDAREA' ], sort=False)['elapsed_workdays'].aggregate(np.sum)\n",
    "\n",
    "dfnew=dfnew.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean time taken per ssl for newdf\n",
    "time = dfnew.groupby([\"ssl\"],  sort= False)['elapsed_workdays'].aggregate(np.mean).round(3)\n",
    "time = time.reset_index()\n",
    "time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfnew.shape)\n",
    "dfnew.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop total elapsed_workdays column because we are using mean elapsed_workdays per ssl as outcome\n",
    "dfnew = dfnew.drop(['elapsed_workdays'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummies for all agencies\n",
    "agencies = pd.get_dummies(dfnew.GroupName)\n",
    "agencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the two dataframes\n",
    "dfnew = pd.concat([dfnew,agencies], axis=1)\n",
    "dfnew.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby the agencies across the same SSLs to show all the agencies permits from an SSL have been to \n",
    "dfnew = dfnew.groupby(['ssl', 'Ward','use_type', 'PRICE', 'QUALIFIED', 'EYB', 'SALE_NUM', 'LIVING_GBA', 'USECODE',\n",
    "       'LANDAREA' ], sort=False)['CFA Review', 'Chinatown Review', 'DC Water Review', 'DDOT Review',\n",
    "       'DOEE', 'DOH Review', 'EISF Review', 'Electrical Review',\n",
    "       'Elevator Review', 'Energy Review', 'Fire Review', 'Green Review',\n",
    "       'HPRB Review', 'Mechanical Review', 'NCPC Review', 'Plumbing Review',\n",
    "       'Structural Review', 'WMATA Review', 'White House Review',\n",
    "       'Zoning Review'].aggregate(np.sum)\n",
    "\n",
    "dfnew = dfnew.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT USING BUT GOOD CODE\n",
    "#pivot the dataframe on sister Agencies denoted by 'groupname' to create columns for all agency types\n",
    "#dfnew = pd.pivot_table(dfnew, values = 'elapsed_workdays', index=['ssl', 'Ward', 'use_type', 'PRICE', 'QUALIFIED', 'EYB', 'SALE_NUM', 'LIVING_GBA', 'USECODE',\n",
    "       #'LANDAREA'], columns = 'GroupName').reset_index().fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfnew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(dfnew.ssl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find duplicate rows based on 'ssl'\n",
    "dfnew[dfnew.duplicated(['ssl'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates by index because few duplicates\n",
    "dfnew.drop([133,404,465,571,887,1296], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the newdf dataframe with max reviewCycle db and total time taken for a permit db, keeping ssls from newdf only\n",
    "dfnew = pd.merge(dfnew, maxRC, on='ssl', how='left')\n",
    "dfnew = pd.merge(dfnew, time, on='ssl', how='left')\n",
    "#dat = dat.drop(['Ward'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfnew.shape)\n",
    "dfnew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.groupby('SALE_NUM').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfnew.groupby('USECODE').size()\n",
    "dfnew.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature and target\n",
    "y = dfnew['elapsed_workdays']\n",
    "X = dfnew.drop(['ssl', 'elapsed_workdays'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log-transform variables with large numeric values \n",
    "import math\n",
    "X['PRICE'] = X['PRICE'].apply(lambda x: math.log(x,10) if x > 0 else x).round(3)\n",
    "X['LIVING_GBA'] = X['LIVING_GBA'].apply(lambda x: math.log(x,10) if x > 0 else x).round(3)\n",
    "X['LANDAREA'] = X['LANDAREA'].apply(lambda x: math.log(x,10) if x > 0 else x).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change EYB to elapsed years from construction to make it continuous\n",
    "\n",
    "X['EYB'] = 2018 - X['EYB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "CV = [ 'Ward','use_type', 'QUALIFIED', 'SALE_NUM', 'USECODE' ]\n",
    "for v in CV:\n",
    "    X[v] = lb.fit_transform(X[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.groupby(['elapsed_workdays']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfnew.groupby(['elapsed_workdays']).size()\n",
    "time_median = dfnew['elapsed_workdays'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y.groupby(\"elapsed_workdays\").size()\n",
    "np.percentile(y, [5, 25, 50, 75, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace outliers beore 5th percentile and beyond 95th percentile by median elapsed_workdays value\n",
    "y = y.apply(lambda x:time_median if x > 66 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.apply(lambda x:time_median if x < 6 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(y, [25, 50, 75, 95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress chaining warning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making new target array based on the mean time a permit takes by weeks\n",
    "y7 = y.apply(lambda x: 1 if x > 7 else 0)\n",
    "y14 = y.apply(lambda x: 1 if x > 14 else 0)\n",
    "y21 = y.apply(lambda x: 1 if x > 21 else 0)\n",
    "y30 = y.apply(lambda x: 1 if x > 30 else 0)\n",
    "\n",
    "#50th percentile\n",
    "#y['elapsed_workdays_10'] = y['elapsed_workdays'].apply(lambda x: 1 if x > 10 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y7.groupby(y7).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y14.groupby(y14).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y21.groupby(y21).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y30.groupby(y30).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the distribution of data in the target array, it looks like 21 and 30 are good thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt   #Data visualisation libraries \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into test and trains sets\n",
    "\n",
    "X_train, X_test, y21_train, y21_test = train_test_split(X, y21, test_size=0.3, random_state=101)\n",
    "X_train, X_test, y30_train, y30_test = train_test_split(X, y30, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGIT MODEL\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l21 = logisticRegr.fit(X_train, np.ravel(y21_train))\n",
    "predictions21 = l21.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l30 = logisticRegr.fit(X_train, np.ravel(y30_train))\n",
    "predictions30 = l30.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy for logistic regression model 21-days: %f' %accuracy_score(np.ravel(y21_test),predictions21))\n",
    "print('accuracy for logistic regression model 30-days: %f' %accuracy_score(np.ravel(y30_test),predictions30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE ROC FOR LOGIT Y=21 DAYS\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit.fit(X_train, y21_train)\n",
    "print('Training score: ', logit.score(X_test, y21_test))\n",
    "y_pred = logit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true positives and false positives for logit for 21 days\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y21_test, y_pred, pos_label=1)\n",
    "rates_logit = pd.DataFrame(dict(fpr=false_positive_rate, tpr=true_positive_rate))\n",
    "roc_auc_logit = auc(rates_logit['fpr'], rates_logit['tpr'])\n",
    "print('AUC: ', roc_auc_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rates_logit.fpr, rates_logit.tpr, 'b', label = 'logit = %0.3f' % roc_auc_logit)\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic for Logit')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC FOR 30 DAYS\n",
    "logit = LogisticRegression()\n",
    "logit.fit(X_train, y30_train)\n",
    "print('Training score: ', logit.score(X_test, y30_test))\n",
    "y_pred = logit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true positives and false positives for logit for 30 days\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y30_test, y_pred, pos_label=1)\n",
    "rates = pd.DataFrame(dict(fpr=false_positive_rate, tpr=true_positive_rate))\n",
    "roc_auc = auc(rates['fpr'], rates['tpr'])\n",
    "print('AUC: ', roc_auc)\n",
    "\n",
    "\n",
    "plt.plot(rates.fpr, rates.tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic for Logit')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score, validation_curve, learning_curve, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create knn model with k=1\n",
    "k1 = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "#k17 = k1.fit(X_train, np.ravel(y7_train))\n",
    "k121 = k1.fit(X_train, np.ravel(y21_train))\n",
    "k130 = k1.fit(X_train, np.ravel(y30_train))\n",
    "\n",
    "#Y_k17_predict = k17.predict(X_test)\n",
    "Y_k121_predict = k121.predict(X_test)\n",
    "Y_k130_predict = k130.predict(X_test)\n",
    "\n",
    "#print('k-NN accuracy for test set k=1: %f' % accuracy_score(y7_test,Y_k17_predict))\n",
    "print('k-NN accuracy for test set k=1 and 21 days data: %f' % accuracy_score(y21_test,Y_k121_predict))\n",
    "print('k-NN accuracy for test set k=1 and 30 days data: %f' % accuracy_score(y30_test,Y_k130_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create knn model with k=7\n",
    "k7 = KNeighborsClassifier(n_neighbors = 7)\n",
    "\n",
    "#k77 = k7.fit(X_train, np.ravel(y7_train))\n",
    "k721 = k7.fit(X_train, np.ravel(y21_train))\n",
    "k730 = k7.fit(X_train, np.ravel(y30_train))\n",
    "\n",
    "#Y_k77_predict = k77.predict(X_test)\n",
    "Y_k721_predict = k721.predict(X_test)\n",
    "Y_k730_predict = k730.predict(X_test)\n",
    "\n",
    "#print('k-NN accuracy for test set k=7: %f' % accuracy_score(y7_test,Y_k77_predict))\n",
    "print('k-NN accuracy for test set k=7 and 21 days data: %f' % accuracy_score(y21_test,Y_k721_predict))\n",
    "print('k-NN accuracy for test set k=7 and 30 days data: %f' % accuracy_score(y30_test,Y_k730_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create knn model with k=15\n",
    "k15 = KNeighborsClassifier(n_neighbors = 15)\n",
    "\n",
    "#k15_7 = k15.fit(X_train, np.ravel(y7_train))\n",
    "k15_21 = k15.fit(X_train, np.ravel(y21_train))\n",
    "k15_30 = k15.fit(X_train, np.ravel(y30_train))\n",
    "\n",
    "#Y_k15_7_predict = k15_7.predict(X_test)\n",
    "Y_k15_21_predict = k15_21.predict(X_test)\n",
    "Y_k15_30_predict = k15_30.predict(X_test)\n",
    "\n",
    "#print('k-NN accuracy for test set k=15: %f' % accuracy_score(y7_test,Y_k15_7_predict))\n",
    "print('k-NN accuracy for test set k=15 and 21 days data: %f' % accuracy_score(y21_test,Y_k15_21_predict))\n",
    "print('k-NN accuracy for test set k=15 and 30 days data: %f' % accuracy_score(y30_test,Y_k15_30_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-fold cross validation\n",
    "#k=1\n",
    "#score_k1_7 = cross_val_score(k17,X, np.ravel(y7), cv = 10)\n",
    "score_k1_21 = cross_val_score(k121,X, np.ravel(y21), cv = 10)\n",
    "score_k1_30 = cross_val_score(k130,X, np.ravel(y30), cv = 10)\n",
    "\n",
    "#print('the avg score for k1_7 model is: %f' %np.mean(score_k1_7))\n",
    "print('the avg score for k1_21 model is: %f' %np.mean(score_k1_21))\n",
    "print('the avg score for k1_30 model is: %f' %np.mean(score_k1_30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=7\n",
    "#score_k7_7 = cross_val_score(k77,X, np.ravel(y7), cv = 10)\n",
    "score_k7_21 = cross_val_score(k721,X, np.ravel(y21), cv = 10)\n",
    "score_k7_30 = cross_val_score(k730,X, np.ravel(y30), cv = 10)\n",
    "\n",
    "#print('the avg score for k7_7 model is: %f' %np.mean(score_k7_7))\n",
    "print('the avg score for k7_21 model is: %f' %np.mean(score_k7_21))\n",
    "print('the avg score for k7_30 model is: %f' %np.mean(score_k7_30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=15\n",
    "#score_k15_7 = cross_val_score(k15_7,X, np.ravel(y7), cv = 10)\n",
    "score_k15_21 = cross_val_score(k15_21,X, np.ravel(y21), cv = 10)\n",
    "score_k15_30 = cross_val_score(k15_30,X, np.ravel(y30), cv = 10)\n",
    "\n",
    "#print('the avg score for k15_7 model is: %f' %np.mean(score_k15_7))\n",
    "print('the avg score for k15_21 model is: %f' %np.mean(score_k15_21))\n",
    "print('the avg score for k15_30 model is: %f' %np.mean(score_k15_30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation curve for cv=10 using k15 fitted model for k values 6 to 42\n",
    "k_range = np.arange(6,55,3 )\n",
    "train_scores, test_scores = validation_curve(k15_21, X, y21, param_name='n_neighbors',\n",
    "                                            param_range=k_range, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "print(test_mean)\n",
    "print(train_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.arange(101)\n",
    "max_depth = np.arange(6,55,3)\n",
    "plt.plot(max_depth, train_mean, label='Train')\n",
    "plt.plot(max_depth, test_mean, label='Test')\n",
    "plt.xlabel('Maximum K')\n",
    "plt.ylabel('Score')\n",
    "plt.title('KNN Validation Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks best for k=10 for y=21 days/3 weeks target variable\n",
    "\n",
    "#k=10\n",
    "k10 = KNeighborsClassifier(n_neighbors = 10)\n",
    "k10.fit(X_train, np.ravel(y21_train))\n",
    "Y_k10_predict = k10.predict(X_test)\n",
    "print('k-NN accuracy for test set k=10: %f' % accuracy_score(y21_test,Y_k10_predict))\n",
    "\n",
    "score_k10 = cross_val_score(k10, X, np.ravel(y21), cv = 10)\n",
    "print(score_k10)\n",
    "print('the avg score for k10 model is: %f' %np.mean(score_k10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=50\n",
    "k50 = KNeighborsClassifier(n_neighbors = 50)\n",
    "k50.fit(X_train, np.ravel(y21_train))\n",
    "Y_k50_predict = k50.predict(X_test)\n",
    "print('k-NN accuracy for test set k=50: %f' % accuracy_score(y21_test,Y_k50_predict))\n",
    "\n",
    "score_k50 = cross_val_score(k50, X, np.ravel(y21), cv = 10)\n",
    "print(score_k50)\n",
    "print('the avg score for k50 model is: %f' %np.mean(score_k50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true positives and false positives for knn y-21 days model and k=10\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y21_test, Y_k10_predict, pos_label=1)\n",
    "rates_knn = pd.DataFrame(dict(fpr=false_positive_rate, tpr=true_positive_rate))\n",
    "roc_auc_knn = auc(rates_knn['fpr'], rates_knn['tpr'])\n",
    "print('AUC: ', roc_auc_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rates_knn.fpr, rates_knn.tpr, 'g', label = 'KNN = %0.3f' % roc_auc_knn)\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic k-10 y-21days')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true positives and false positives for knn y-21 days model and k=50\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y21_test, Y_k50_predict, pos_label=1)\n",
    "rates_knn = pd.DataFrame(dict(fpr=false_positive_rate, tpr=true_positive_rate))\n",
    "roc_auc = auc(rates_knn['fpr'], rates_knn['tpr'])\n",
    "print('AUC: ', roc_auc)\n",
    "\n",
    "plt.plot(rates_knn.fpr, rates_knn.tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic k-50 y-21days')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian NB\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "#g7 = gnb.fit(X, y7)\n",
    "g21 = gnb.fit(X, y21)\n",
    "g30 = gnb.fit(X, y30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.mean(cross_val_score(g7, X, y7, cv=10)))\n",
    "print(np.mean(cross_val_score(g21, X, y21, cv=10)))\n",
    "print(np.mean(cross_val_score(g30, X, y30, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y21_train)\n",
    "print('Training score: ', gnb.score(X_test, y21_test))\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true positives and false positives for GNB 21 days\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y21_test, y_pred, pos_label=1)\n",
    "rates_gnb = pd.DataFrame(dict(fpr=false_positive_rate, tpr=true_positive_rate))\n",
    "roc_auc_gnb = auc(rates_gnb['fpr'], rates_gnb['tpr'])\n",
    "print('AUC_GNB: ', roc_auc_gnb)\n",
    "\n",
    "plt.plot(rates_gnb.fpr, rates_gnb.tpr, 'r', label = 'GNB = %0.3f' % roc_auc_gnb)\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic GNB 21 days')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree\n",
    "import graphviz\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(criterion = \"gini\", max_depth=4)\n",
    "\n",
    "#m7 = model.fit(X_train,y7_train)\n",
    "m21 = model.fit(X_train,y21_train)\n",
    "m30 = model.fit(X_train,y30_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction7 = m7.predict(X_test)\n",
    "prediction21 = m21.predict(X_test)\n",
    "prediction30 = m30.predict(X_test)\n",
    "\n",
    "#print('DT accuracy for test set 7-days: %f' % accuracy_score(y7_test,prediction7))\n",
    "print('DT accuracy for test set 21-days: %f' % accuracy_score(y21_test,prediction21))\n",
    "print('DT accuracy for test set 30-days: %f' % accuracy_score(y30_test,prediction30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.mean(cross_val_score(model, X, ybin, cv=3)))\n",
    "#print(np.mean(cross_val_score(model, X, ybin, cv=5)))\n",
    "\n",
    "#print(np.mean(cross_val_score(m7, X, y7, cv=10)))\n",
    "print(np.mean(cross_val_score(m21, X, y21, cv=10)))\n",
    "print(np.mean(cross_val_score(m30, X, y30, cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true positives and false positives for DT y-21 days model\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y21_test, prediction21, pos_label=1)\n",
    "rates_dtm = pd.DataFrame(dict(fpr=false_positive_rate, tpr=true_positive_rate))\n",
    "roc_auc_dtm = auc(rates_dtm['fpr'], rates_dtm['tpr'])\n",
    "print('AUC DTM: ', roc_auc_dtm)\n",
    "\n",
    "plt.plot(rates_dtm.fpr, rates_dtm.tpr, 'c', label = 'AUC = %0.3f' % roc_auc_dtm)\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic DT 21 days')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree model for 21 and 30 days in separate file\n",
    "import pydotplus\n",
    "\n",
    "dot_data21 = tree.export_graphviz(m21, out_file=None, feature_names=X.columns, filled=True,rounded=True)                                \n",
    "graph21 = graphviz.Source(dot_data21)\n",
    "graph21.render(\"SSL data tree\")\n",
    "\n",
    "pydot_graph = pydotplus.graph_from_dot_data(dot_data21)\n",
    "pydot_graph.write_png('original_tree.png')\n",
    "#graph21.savefig('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot variable importance for 14-days model\n",
    "var_imp = pd.DataFrame({'Variable': X.columns, \n",
    "                        'Importance': m21.feature_importances_})\n",
    "var_imp = var_imp.sort_values(by='Importance', ascending=False)\n",
    "var_imp = var_imp[:-18].sort_values(by='Importance')\n",
    "var_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(5, 3.5))\n",
    "plt.barh(var_imp['Variable'], var_imp['Importance'])\n",
    "plt.xlabel('Proportion')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()\n",
    "fig1.savefig('vim.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfmodel = RandomForestClassifier(n_estimators=100, criterion = \"entropy\",max_depth=5, random_state=0)\n",
    "#rf7 = rfmodel.fit(X, y7)\n",
    "rf21 = rfmodel.fit(X, y21)\n",
    "rf30 = rfmodel.fit(X, y30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.mean(cross_val_score(rf7, X, y7, cv=10)))\n",
    "print(np.mean(cross_val_score(rf21, X, y21, cv=10)))\n",
    "print(np.mean(cross_val_score(rf30, X, y30, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = RandomForestClassifier(n_estimators=100, criterion = \"entropy\",random_state=0)\n",
    "rfm.fit(X_train, y21_train)\n",
    "print('Training score: ', rfm.score(X_test, y21_test))\n",
    "y_pred = rfm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true positives and false positives for rfm y-21 days model\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y21_test, y_pred, pos_label=1)\n",
    "rates_rfm = pd.DataFrame(dict(fpr=false_positive_rate, tpr=true_positive_rate))\n",
    "roc_auc_rfm = auc(rates_rfm['fpr'], rates_rfm['tpr'])\n",
    "print('AUC RFM: ', roc_auc_rfm)\n",
    "\n",
    "plt.plot(rates_rfm.fpr, rates_rfm.tpr, 'y', label = 'AUC = %0.3f' % roc_auc_rfm)\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic RFC 21 days')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = rfmodel.estimators_[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(estimator, out_file='rftree.dot', \n",
    "                feature_names = X.columns,\n",
    "                \n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree in separate file\n",
    "dot_data = RandomForestClassifier.export_graphviz(model, out_file=None, feature_names=X.columns)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"cama\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all ROCs in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(6, 3.5))\n",
    "\n",
    "\n",
    "plt.plot(rates_knn.fpr, rates_knn.tpr, 'g', label = 'KNN = %0.3f' % roc_auc_knn)\n",
    "\n",
    "plt.plot(rates_gnb.fpr, rates_gnb.tpr, 'r', label = 'GNB = %0.3f' % roc_auc_gnb)\n",
    "plt.plot(rates_logit.fpr, rates_logit.tpr, 'b', label = 'logit = %0.3f' % roc_auc_logit)\n",
    "\n",
    "plt.plot(rates_dtm.fpr, rates_dtm.tpr, 'c', label = 'DTM = %0.3f' % roc_auc_dtm)\n",
    "\n",
    "#plt.plot(rates_rfm.fpr, rates_rfm.tpr, 'y', label = 'RFM = %0.3f' % roc_auc_rfm)\n",
    "#ax = plt.add_axes([0,0,1,1])\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC for t=21 days')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()\n",
    "fig1.savefig('ROC2.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
